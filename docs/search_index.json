[
["ch4.html", "Chapter 4 Simulation and Randomization Chapter Overview Before You Begin 4.1 Layout of this chapter 4.2 Introducing Randomness 4.3 Reproducing randomness 4.4 Replication 4.5 Function Writing 4.6 Summarization 4.7 Simulation-Based Examples 4.8 Resampling-Based Examples 4.9 Exercise 4", " Chapter 4 Simulation and Randomization Chapter Overview Simulation modeling is one of the primary reasons to move away from spreadsheet-type programs (like Microsoft Excel) and into a program like R. R allows you to replicate the same (possibly complex and detailed) calculations over and over with slightly different random values. You can then summarize and plot the results of these replicated calculations all within the same program. Analyses of this type are Monte Carlo methods: they randomly sample from a set of quanities for the purpose of generating and summarizing a distribution of some statistic related to the sampled quantities. If this concept is confusing, hopefully this chapter will clarify. In this chapter, you will learn the basic skills needed for simulation (i.e., Monte Carlo) modeling in R including: creation of random deviates for loops more advanced function writing summarization of many values from a distribution IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapters 1 or 2, you are recommended to walk through the material found in those chapters before proceeding to this material. Additionally, you will find the material in Section 3.3 helpful for this chapter. Remember that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this document. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch4.R and save it in the directory C:/Users/YOU/Documents/R-Workshop/Chapter4. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. 4.1 Layout of this chapter This chapter is divided into two main sections: Required material (Sections 4.2 and 4.6) which is necessary to understand the examples in this chapter and the subsequent chapters Example Cases (Sections 4.7 and 4.8) which apply the skills learned in the required material. In the workshop sessions, you will walkthrough 2-3 of these example cases at the choice of the group of the participants. If you are interested in simulation modeling, you are suggested to work through all of the example cases, as slightly different tricks will be shown in the different examples. 4.2 Introducing Randomness A critical part of simulation modeling is the use of random processes. A random process is one that generates a different outcome according to some rules each time it is executed. They are tightly linked to the concept of uncertainty: you are unsure about the outcome the next time the process is executed. There are two basic ways to introduce randomness in R: random deviates and resampling. 4.2.1 Random deviates In Section 3.3, you learned about using probability distributions in R. One of the uses was the r- family of distribution functions. These functions create random numbers following a random process specified by a probability distribution. Consider animal survival as an example. At the end of each year, each individual alive at the start can either live or die. There are two outcomes here, and suppose each animal has an 80% chance of surviving. The number of individuals that survive is the result of a binomial random process in which there were \\(n\\) individuals alive at the start of this year and \\(p\\) is the probability that any one individual survives to the next year. You can execute one binomial random process where \\(p = 0.8\\) and \\(n = 100\\) like this: rbinom(n = 1, size = 100, prob = 0.8) ## [1] 78 The result you get will almost certainly be different from the one printed here. That is the random component. You can execute many such binomial processes by changing the n argument. Plot the distribution of expected surviving individuals: survivors = rbinom(1000, 100, 0.8) hist(survivors, col = &quot;skyblue&quot;) Another random process is the lognormal process: it generates random numbers such that the log of the values are normally-distributed with mean equal to logmean and standard deviation equal to logsd: hist(rlnorm(1000, 0, 0.1), col = &quot;skyblue&quot;) There are many random processes you can use in R. Checkout Table ?? for more examples as well as the help files for each individual function for more details. 4.2.2 Resampling Using random deviates works great for creating new random numbers, but what if you already have a set of numbers that you wish to introduce randomness to? For this, you can use resampling techniques. In R, the sample function is used to sample size elements from the vector x: sample(x = 1:10, size = 5) ## [1] 7 4 6 10 5 You can sample with replacement (where it is possible to sample the same element two or more times): sample(x = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), size = 10, replace = T) ## [1] &quot;c&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; You can set probabilities on the sampling of different elements1: sample(x = c(&quot;live&quot;, &quot;die&quot;), size = 10, replace = T, prob = c(0.8, 0.2)) ## [1] &quot;live&quot; &quot;die&quot; &quot;live&quot; &quot;live&quot; &quot;die&quot; &quot;die&quot; &quot;live&quot; &quot;live&quot; &quot;live&quot; &quot;live&quot; Notice that this is the same as the binomial random process above, but with only 10 trials and the printing of the outcomes rather than the number of successes. 4.3 Reproducing randomness For reproducibility purposes, you may wish to get the same exact random numbers each time you run your script. To do this, you need to set the random seed, which is the starting point of the random number generator your computer uses. If you run these two lines of code, you should get the same result as printed here: set.seed(1234) rnorm(1) ## [1] -1.207066 4.4 Replication To use Monte Carlo methods, you need to be able to replicate some random process many times. There are two main ways this is commonly done: either withreplicate or with for loops. 4.4.1 replicate The replicate function executes some expression many times and returns the output from each execution. Say we have a vector x, which represents 30 observations of fish length (mm): x = rnorm(30, 500, 30) We wish to build the sampling distribution of the mean length “by hand”. We can sample randomly from it, calculate the mean, then repeat this process many times: means = replicate(n = 1000, expr = { x_i = sample(x, length(x), replace = T) mean(x_i) }) If we take mean(means) and sd(means), that should be very similar to mean(x) and se(x). Create the se function (also shown in Section 2.11) and prove this to yourself: se = function(x) sd(x)/sqrt(length(x)) mean(means); mean(x) ## [1] 493.8096 ## [1] 493.4166 sd(means); se(x) ## [1] 4.899929 ## [1] 5.044153 4.4.2 The for loop In programming, a loop is a command that does something over and over until it reaches some point that you specify. R has a few types of loops: repeat, while, and for, to name a few. for loops are among the most common in simulation modeling. A for loop repeats some action for however many times you tell it for each value in some vector. The syntax is: for (var in seq) { expression(var) } The loop calculates the expression for values of var for each element in the vector seq. For example: for (i in 1:5) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 The print command will be executed 5 times: once for each value of i. It is the same as: i = 1; print(i); i = 2; print(i); i = 3; print(i); i = 4; print(i); i = 5; print(i) If you remove the print() function, see what happens: for (i in 1:5) { i^2 } Nothing is printed to the console. R did the calculation, but did not show you or store it. Often, you’ll need to store the results of the calculation in a container object: results = numeric(5) This makes an empty numeric vector of length 5 that are all 0’s. You can store the output of your loop calculations in results: for (i in 1:5) { results[i]=i^2 } results ## [1] 1 4 9 16 25 When i^2 is calculated, it will be placed in the element results[i]. This was a trivial example, because you should do things like this using R’s vectorized calculation framework: (1:5)^2 (see Section 1.6). However, there are times where it is advantageous to use a loop. Particularly in cases where: the calculations in one element are determined from the value in previous elements, such as in time series models the calculations have multiple steps you wish to store multiple results you wish to track the progress of your calculations As an illustration for item (1) above, build a (very) basic population model. At the start of the first year, the population abundance is 1000 individuals and grows by an average factor of 1.1 per year (reproduction and death processes result in a growth rate of 10%) before harvest. The growth rate varies randomly, however. Each year, the 1.1 growth factor has variability introduced by small changes in survival and reproductive process. Model these variations as lognormal random variables. After production, 8% of the population is harvested. Simulate and plot the abundance at the end of the year for 100 years: nt = 100 # number of years N = NULL # container for abundance N[1] = 1000 # first end-of-year abundance for (t in 2:nt) { # N this year is N last year * growth * # randomness * fraction that survive harvest N[t] = (N[t-1] * 1.1 * rlnorm(1, 0, 0.1)) * (1 - 0.08) } # plot plot(N, type = &quot;l&quot;, pch = 15, xlab = &quot;Year&quot;, ylab = &quot;Abundance&quot;) Examples of the other three utilities are shown in the example cases. 4.5 Function Writing In Monte Carlo analyses, it is often useful to wrap code into functions. This makes them easy to be replicated and have the settings adjusted. As an example, turn the population model shown above into a function: pop_sim = function(nt, grow, sd_grow, U, plot = F) { N = NULL N[1] = 1000 for (t in 2:nt) { N[t] = (N[t-1] * grow * rlnorm(1, 0, sd_grow)) * (1 - U) } if (plot) { plot(N, type = &quot;l&quot;, pch = 15, xlab = &quot;Year&quot;, ylab = &quot;Abundance&quot;) } N } This function takes five inputs: nt: the number of years, grow: the population growth rate, sd_grow: the amount of annual variability in the growth rate U: the annual exploitation rate plot: whether you wish to have a plot created. It has a default setting of FALSE: if you don’t specify plot = T when you call pop_sim, you won’t see a plot made. It returns one output: the vector of population abundance. Use your function once using the same settings as before: pop_sim(100, 1.1, 0.1, 0.08, T) ## [1] 1000.0000 991.9189 966.1673 1119.8166 1167.4864 940.7572 1003.4028 ## [8] 993.7741 1092.3542 1114.6248 1011.6523 1012.8149 1180.6373 1125.6400 ## [15] 1128.9152 1234.7466 1546.7473 1438.8662 1677.2920 1878.7559 1601.6186 ## [22] 1803.7180 2124.3262 1888.6512 1950.4458 2170.6249 2574.6142 2088.5542 ## [29] 2506.9833 2743.3092 2350.9056 2150.1135 2067.0640 1870.8924 2061.8156 ## [36] 1895.1914 1690.9365 2112.3612 2114.7401 1815.7680 1949.8661 1953.3604 ## [43] 1810.1937 1982.6345 1920.9718 1970.1317 1905.9889 2061.5250 2105.1941 ## [50] 1870.4381 1809.3494 1717.6727 1396.3874 1311.3019 1288.7084 1472.9669 ## [57] 1681.8080 1636.4883 1874.0338 1937.6544 1767.9563 1927.9457 2056.8504 ## [64] 1868.7910 2133.6614 2393.3918 2174.1558 1987.7674 2079.1235 1990.4753 ## [71] 2020.4988 2207.8347 2371.1186 2235.4894 1861.8581 1955.1694 2126.6954 ## [78] 1934.2892 1919.1523 2035.2788 2595.0493 2624.3191 2634.0353 2619.8963 ## [85] 3063.5877 3335.9375 3036.8140 3211.4930 3108.6992 2879.1979 3044.2377 ## [92] 2667.8093 3036.1608 3264.0875 3067.2281 3003.3483 2867.4153 2882.2673 ## [99] 2741.6617 2748.9291 Now, you wish to replicate exectuing this function 1000 times. Use the replicate function to do this: out = replicate(n = 1000, expr = pop_sim(100, 1.1, 0.1, 0.08, F)) If you do dim(out), you’ll see that rows are stored as years (there are 100 of them) and columns are stored as replicates (there are 1000 of them). Notice how wrapping the code in the function made the replicate call easy. Here are some advantages of wrapping code like this into a function: If you do the same task over and over, you don’t need to type all of the code to perform the task, just the function call. If you need to change the way the function behaves (mechanically in the function body), you only need to change it one place: in the function definition. You can easily change the settings of the code (e.g., whether you want to see the plot) in one place Function writing can lead to shorter scripts Function writing can lead to more readable code (if people know what your functions do) 4.6 Summarization After replicating a calculation many times, you will need to summarize the results. Here are several examples using the out matrix. 4.6.1 Central Tendency You can calculate the mean abundance each year across your iterations using the apply function (Section 1.9): N_mean = apply(out, 1, mean) N_mean[1:10] ## [1] 1000.000 1017.511 1033.436 1054.619 1069.542 1086.413 1107.854 ## [8] 1129.619 1145.288 1161.254 You could do the same thing using median rather than mean. Mode is more difficult to calculate in R, if you need to get the mode, try to Google it2. 4.6.2 Variability One of the primary reasons to conduct a Monte Carlo analysis is to obtain estimates of variability. You can summarize the variability easily using the quantile function: N_quants = apply(out, 1, function(x) quantile(x, c(0.1, 0.9))) plot(N_mean, type = &quot;l&quot;, ylim = c(0, 10000)) lines(N_quants[1,], lty = 2) lines(N_quants[2,], lty = 2) Notice how a user-defined function was passed to apply. The range within the two dashed lines represents the range that encompassed the central 80% of the random abundances each year. 4.6.3 Frequencies Often you will want to count how many times something happened. In some cases, the fraction of times something happened can be interpretted as a probability. The table function is very useful for counting occurences of events. Suppose you are interested in how many of your iterations resulted in fewer than 1000 individuals at year 10: out10 = ifelse(out[10,] &lt; 1000, &quot;less10&quot;, &quot;greater10&quot;) table(out10) ## out10 ## greater10 less10 ## 643 357 Suppose you are also interested in how many of your iterations resulted in fewer than 1100 individuals at year 20: out20 = ifelse(out[20,] &lt; 1100, &quot;less20&quot;, &quot;greater20&quot;) table(out20) ## out20 ## greater20 less20 ## 592 408 Now suppose you are interested in how these two metrics are related: table(out10, out20) ## out20 ## out10 greater20 less20 ## greater10 502 141 ## less10 90 267 As an example in iterpretting this output, most often populations that were greater than 1000 at year 10 were also greater than 1100 at year 20. If a population was less than 1000 at year 10, it was more likely to be less than 1100 at year 20 than to be greater than it. You can turn these into probabilities (if you believe your model represents reality) by dividing each cell by the total number of iterations: round(table(out10, out20)/1000, 2) ## out20 ## out10 greater20 less20 ## greater10 0.50 0.14 ## less10 0.09 0.27 4.7 Simulation-Based Examples 4.7.1 Test rnorm In this example, you will verify that the function rnorm works the same way that qnorm and pnorm work. Hopefully it will also reinforce the way the random, quantile, and cumulative distribution functions work in R. If you’re feeling advanced, try adjusting this example for another distribution like the Poisson. First, specify the mean and standard deviation for this example: mu = 500; sig = 30 Now make up n (any number of your choosing, something greater than 10) random deviates from this normal distribution: random = rnorm(100, mu, sig) Test the quantiles (obtain the values that p * 100% of the quantities fall below, both for random numbers and from the qnorm function): p = seq(0.01, 0.99, 0.01) random_q = quantile(random, p) normal_q = qnorm(p, mu, sig) plot(normal_q ~ random_q); abline(c(0,1)) The fact that all the quantiles fall around the 1:1 line sugggests the n random samples samples are indeed from a normal distribution. Any variabilities you see are due to sampling errors. If you increase n to n = 1e6 (one million), you’ll see no deviations. This is called a q-q plot, and is frequently used to assess the fit of data to a distribution. Now test the values in their agreement with the pnorm function. Plot the cumulative density functions for the truely normal curve and that approximated by the random deviates: q = seq(400, 600, 10) random_cdf = ecdf(random) random_p = random_cdf(q) normal_p = pnorm(q, mu, sig) plot(normal_p ~ q, type = &quot;l&quot;, col = &quot;blue&quot;) points(random_p ~ q, col = &quot;red&quot;) The ecdf function obtains the empirical cumulative density function (which is just pnorm for a sample). It allows you to plug in any random variable and obtain the probability of having one less than it. 4.7.2 Stochastic Power Analysis A power analysis is one where the analyst wishes to determine how much power they will have to detect an effect. Having high power means ensuring you do not falsely reject a true hypothesis (e.g., claiming that there is not effect based on a p-value greater than 0.05). You can conduct a power analysis using stochastic simulation. Here, you will write a power analysis to determine how likely are you to be able to correctly identify what you deem to be a biologically-meaningful difference in survival between two tagging procedures. You know one tagging procedure has approximately a 10% mortality rate (10% of tagged fish die within the first 12 hours as result of the tagging process). Another, cheaper and less labor intensive method has been proposed but before implementing it, your agency wishes to determine if it will have a meaningful impact on the reliability of the study. You and your colleagues determine that if the mortality rate reaches 25%, then gains in time and cost efficiency would be offset by needing to tag more fish (because more will die). You have decided to perform a small-scale study to determine if the new method affects mortality enough to result in 25% or more mortality. The study will tag n individuals using each method (new and old) and track the fraction that survived after 12 hours. Before performing the study however, you deem it important to determine how large n needs to be to answer this question. You decide to use a stochastic power analysis based on what you’ve learned in this book to help your research group. The small-scale study can tag a total of at most 100 fish with the currently available resources. Could you tag fewer than 100 total individuals and still have a high probability of correctly identifying an effect of this size? The stochastic power analysis approach works like this (this is called psuedocode): Simulate data under the reality that the difference is real with n observations per treatment, where n &lt; 30/2. Fit the model that will be used when the real data are collected. Determine if an effect at least as large as the effect of interest was detected with a significant p-value Replicate steps 1 - 3 many times. Determine what fraction of times the effect was correctly identified. Replicate step 4 while varying n over the interval from 5 to 15. Step 2 will require fitting a generalized linear model, for a review revisit Section 3.2 (specifically Section 3.2.1 on logistic regression). First, create a function that will generate data, fit the model, and determine if the p-value is significant (steps 1-3 above): sim_fit = function(n, p_old = 0.10, p_new = 0.25) { ### step 1: create the data ### # generate random response data dead_old = rbinom(n, size = 1, prob = p_old) dead_new = rbinom(n, size = 1, prob = p_new) # create the predictor variable method = rep(c(&quot;old&quot;, &quot;new&quot;), each = n) # create a data.frame to pass to glm df = data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method = relevel(df$method, ref = &quot;old&quot;) ### step 2: fit the model ### fit = glm(dead ~ method, data = df, family = binomial) ### step 3: determine if a sig. p-value was found ### # extract the p-value pval = summary(fit)$coef[2,4] # determine if it was found to be significant pval &lt; 0.05 } Next, for steps 4 and 5, set up a nested for loop. This will have two loops: one that loops over sample sizes for step 5 and one that loops over replicates of each sample size (step 4): I = 10 # the number of replicates at each sample size n_try = seq(10, 50, 10) # the test sample sizes N = length(n_try) # count them # container: out = matrix(NA, I, N) # matrix with I rows and N columns for (n in 1:N) { for (i in 1:I) { out[i,n] = sim_fit(n = n_try[n]) } } You now have a matrix of TRUE and FALSE elements that indicates whether a significant difference was found at the \\(\\alpha = 0.05\\) level if the effect was truely as large as you care about. You can obtain the proportion of all the replicates at each sample size that resulted in a significant difference using the mean function with apply: plot(apply(out, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of Finding Effect (Power)&quot;) Even if you tagged 100 fish total, you would have a 50% chance of saying the effect (which truely is there!) is present under the null hypothesis testing framework. Suppose you and your colleagues aren’t relying on p-values in this case, and are purely interested in how precisely the effect size would be estimated. Adapt your function to determine how frequently it is you would be able to estimate the true mortality probability of the new method within +/- 5% based on the point estimate only (the true estimate must be between 0.2 and 0.3 for a successful study). Change your function to calculate this additional metric and re-run the analysis: sim_fit = function(n, p_old = 0.10, p_new = 0.25) { # create the data dead_old = rbinom(n, size = 1, prob = p_old) dead_new = rbinom(n, size = 1, prob = p_new) # create the predictor variable method = rep(c(&quot;old&quot;, &quot;new&quot;), each = n) # create a data.frame to pass to glm df = data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method = relevel(df$method, ref = &quot;old&quot;) # fit the model fit = glm(dead ~ method, data = df, family = binomial) ### step 3: determine if a sig. p-value was found ### # extract the p-value pval = summary(fit)$coef[2,4] # determine if it was found to be significant sig_pval = pval &lt; 0.05 # obtain the estimated mortality rate for the new method p_new_est = predict(fit, data.frame(method = c(&quot;new&quot;)), type = &quot;response&quot;) # determine if it is +/- 5% from the true value prc_est = p_new_est &gt;= (p_new - 0.05) &amp; p_new_est &lt;= (p_new + 0.05) # return a vector with these two elements c(sig_pval = sig_pval, prc_est = unname(prc_est)) } # run the analysis I = 10 # the number of replicates at each sample size n_try = seq(10, 50, 10) # the test sample sizes N = length(n_try) # count them # containers: out_sig = matrix(NA, I, N) # matrix with I rows and N columns out_prc = matrix(NA, I, N) # matrix with I rows and N columns for (n in 1:N) { for (i in 1:I) { tmp = sim_fit(n = n_try[n]) # run sim out_sig[i,n] = tmp[&quot;sig_pval&quot;] # extract and store significance metric out_prc[i,n] = tmp[&quot;prc_est&quot;] # extract and store precision metric } } par(mfrow = c(1,2)) plot(apply(out_sig, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of Finding Effect (Power)&quot;) plot(apply(out_prc, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of A Precise Estimate&quot;) It seems that even if you tagged 50 fish per treatment, you would have a 60% chance of estimating that the mortality rate is between 0.2 and 0.3 if it was truly 0.25. You and your colleagues consider these results and determine that you will need to somehow aquire more funds to tag more fish in the small-scale study in order to a high level of confidence in the results. 4.7.3 Harvest Policy Analysis In this example, you will simulate population dynamics under a more realistic model than in Sections 4.4.2 and 4.5 for the purposes of evaluating different harvest policies. Suppose you are a fisheries research biologist, and a commercial fishery for pink salmon (Oncorhynchus gorbuscha) takes place in your district. For the past 10 years, it has been fished with an exploitation rate of 40% (40% of the fish that return each year have been harvested, exploitation rate is abbreviated by \\(U\\)), resulting in an average annual harvest of 8.5 million fish. The management plan is up for evaluation this year, and your supervisor has asked you to prepare an analysis that determines if more yields could be taken if a different exploitation rate were to be used in the future. Based on historical data, your best understanding implies that the stock is driven by Ricker spawner-recruit dynamics. That is, the total number of fish that return this year (recruits) are a function of the total number of spawners that spawned the recruits this year. The functional form of the Ricker relationship can be written this way: \\[R_t = \\alpha S_{t-1} e^{-\\beta S_{t-1} + \\varepsilon_t} ,\\varepsilon_t \\sim N(0,\\sigma)\\] where \\(\\alpha\\) is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and \\(\\beta\\) is a measure of the strength of density-dependent mortality. Notice that the error term is in the exponent, which makes \\(e^{\\varepsilon_t}\\) lognormal. You have estimates of the parameters: \\(\\alpha = 6\\) \\(\\beta = 1 \\times 10^{-7}\\) \\(\\sigma = 0.4\\) You decide that you can build a policy analysis by simulating the stock forward through time under different exploitation rates to determine if its reasonable to expect a different exploitation rate to provide more yields than what are currently being extracted. First, write a function for your population model. Your function must: take the parameters, dimensions (number of years), and the policy variable (\\(U\\)) as input arguments simulate the population using Ricker dynamics calculate and return the sum of the yields over the number future years you simulated. # Step #1: name function and give it some arguments ricker_sim = function(ny, params, U) { # extract the parameters out by name: alpha = params[&quot;alpha&quot;] beta = params[&quot;beta&quot;] sigma = params[&quot;sigma&quot;] # create containers: # yep, you can do this R = S = H = NULL # initialize the population in the first year # start the population at being fished at 40% # with lognormal error R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma)) S[1] = R[1] * (1 - U) H[1] = R[1] * U # carry simulation forward through time for (y in 2:ny) { # use the ricker function with random lognormal white noise R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma)) #harvest and spawners are the same as before S[y] = R[y] * (1 - U) H[y] = R[y] * U } list( mean_H = mean(H), sd_H = sd(H), cv_H = sd(H)/mean(H), mean_S = mean(S) ) } Use the function once: params = c(alpha = 6, beta = 1e-7, sigma = 0.4) out = ricker_sim(U = 0.4, ny = 20, params = params) #average annual harvest (in millions) round(out$mean/1e6, digits = 2) ## numeric(0) If you completed the stochastic power analysis example (Section 4.7.2), you might see where this is going. You are going to replicate applying a fixed policy many times to a random system. This is the Monte Carlo part of the analysis. The policy part is that you will compare several candidate exploitation rates. This time, set up your analysis using sapply and replicate instead of performing a nested for loop as in previous examples: U_try = seq(0.4, 0.6, 0.01) n_rep = 500 H_out = sapply(U_try, function(u) { replicate(n = n_rep, expr = { ricker_sim(U = u, ny = 20, params = params)$mean_H/1e6 }) }) The nested replicate and sapply are a bit cleaner than a nested for loop, but you have less control over the format of the output. Plot the output of your simulations using a boxplot. To make things easier, give H_out column names representing the exploitation rate: colnames(H_out) = U_try boxplot(H_out, outline = F, xlab = &quot;U&quot;, ylab = &quot;Harvest (Millions of Fish)&quot;, col = &quot;tomato&quot;, las = 1, ylim = c(0, max(H_out))) It appears the stock could produce more harvest than its current 8.5 million fish per year if it was fished harder. However, your supervisors also do not want to see the escapement drop below three-quarters of what it has been in recent history (75% of approximately 13 million fish). They ask you to obtain the expected average annual escapement as well as harvest. You can simply re-run the code above, but extracting $S_mean$ rather than $H_mean$. Call this output $S_out$ and plot it just like harvest (if your curious, this blue color iscol = “skyblue”`): S_out = sapply(U_try, function(u) { replicate(n = n_rep, expr = { ricker_sim(U = u, ny = 20, params = params)$mean_S/1e6 }) }) colnames(S_out) = U_try boxplot(S_out, outline = F, xlab = &quot;U&quot;, ylab = &quot;Escapement (Millions of Fish)&quot;, col = &quot;skyblue&quot;, las = 1, ylim = c(0, max(H_out))) mean(H_out[,U_try == 0.4]) ## [1] 8.544733 After seeing this information, your supervisor realizes they are faced with a trade-off: the stock could produce more with high exploitation rates, but they are concerned about pushing the stock too low for sustainability reasons. They tell you to determine the probability the average escapement would not be pushed below 75% of 13 million at each exploitation rate, as well as the probability that the average annual harvests will be at least 20% greater than they are currently (approximately 8.5 million fish). Given your output, this is easy: # determine if each element meets escapement criterion Smeet = S_out &gt; (0.75 * 13) # determine if each element meets harvest criterion Hmeet = H_out &gt; (1.2 * 8.5) # calculate the probability of each occuring at a given exploitation rate: p_Smeet = apply(Smeet, 2, mean) p_Hmeet = apply(Hmeet, 2, mean) You plot this for your supervisor as follows: # the U levels to highlight on plot plot_U = seq(0.4, 0.6, 0.05) # create an empty plot plot(p_Smeet ~ p_Hmeet, type = &quot;n&quot;, xlab = &quot;Probability of Meeting Harvest Criterion&quot;, ylab = &quot;Probability of Meeting Escapement Criterion&quot;) # add gridlines abline(v = seq(0, 1, 0.1), col = &quot;grey&quot;) abline(h = seq(0, 1, 0.1), col = &quot;grey&quot;) #draw on the tradeoff curve lines(p_Smeet ~ p_Hmeet, type = &quot;l&quot;, lwd = 2) # add points and text for particular U policies points(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], pch = 16, cex = 1.5) text(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2)) Equipped with this analysis, your supervisor plans to go to the policy-makers with the recommendation of adjusting the exploitation rate policy to use \\(U = 0.5\\), because they think it balances the trade-off. Notice how if the status quo was maintained, your model suggests you would have complete certainty of staying where you are now: escapement will remain above 75% of its current level with a 100% chance, but you would have no chance of improving harvests to greater than 20% of their current level. Small increases in the exploitation rate (e.g., from 0.4 to 0.45) have a reasonably large gain in harvest performance, but hardly any losses for the escapement criterion. Your supervisor is willing to live with a 90% chance that the escapement will stay where they desire in order to gain a &gt;80% chance of obtaining the desired amount of increases in harvest. The real utility of using Monte Carlo methods in this example is the ability to calculate the probability of some event you are interested in. There are analytical (i.e., not simulation-based) solutions to predict the annual harvest and escapement from a fixed \\(U\\) from a population with parameters \\(\\alpha\\) and \\(\\beta\\), but by incorporating randomness, you were able to obtain the relative weights of outcomes other than the expectation under the deterministic Ricker model. 4.7.4 Mark-Recapture Evaluation 4.8 Resampling-Based Examples 4.8.1 Permutation Test 4.8.2 The Bootstrap 4.9 Exercise 4 In these exercise, you will be adapting the code written in this chapter to investigate slightly different questions. You should create a new R script Ex4.R in your working directory for these exercises so your chapter code is left unchanged. Exercise 4A is based solely on the required material and Exercises 4B - 4F are based on the example cases. You should work through each example before attempting each of the later exercises. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Exercise 4A: Required Material Only These questions are based on the material in Sections 4.2 - 4.6 only. Solutions Exercise 4B: Stochastic Power Analysis These questions will require you to adapt the code written in Section 4.7.2 What sample size n do you need to have a power of 0.8 of detecting a significant difference between the two tagging methods? How do the inferences from the power analysis change if you are interested in p_new = 0.4 instead of p_new = 0.25? Do you need to tag more or fewer fish in this case? Your analysis takes a bit of time to run so you are interested in tracking its progress. Add a progress message to your nested for loop that will print the sample size currently being analyzed: for (n in 1:N) { cat(&quot;\\r&quot;, &quot;Sample Size = &quot;, n_try[n]) for (i in 1:I) { ... } } Solutions Exercise 4C: Harvest Policy Analysis These questions will require you to adapt the code written in Section 4.7.3 Solutions Exercise 4D: Mark-Recapture Evaluation These questions will require you to adapt the code written in Section 4.7.4 Solutions Exercise 4E: Permutation Tests These questions will require you to adapt the code written in Section 4.8.1 Solutions Exercise 4F: The Bootstrap These questions will require you to adapt the code written in Section 4.8.2 Solutions If prob doesn’t sum to 1, then it will be rescaled: prob = prob/sum(prob)↩ Google is an R programmer’s best friend. There is a massive online community for R, and if you have a question on something, it has almost certainly been asked somewhere on the web.↩ "]
]
