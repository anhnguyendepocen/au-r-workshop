[
["ch3.html", "Chapter 3 Basic Statistics Chapter Overview Before You Begin 3.1 Review: The General Linear Model 3.2 Simple Linear Regression 3.3 ANOVA: Categorical predictors 3.4 ANCOVA: Continuous and categorical predictors 3.5 Interactions 3.6 Model Selection: AIC 3.7 An Example GLM: Logistic Regression 3.8 Probability Distributions", " Chapter 3 Basic Statistics Chapter Overview In this chapter, you will get familiar with the basics of using R for the purpose it was designed: statisitical analysis. You will learn how to: how to fit and interpret the output from various general linear models: simple linear regression models multiple regression models higher order polynomial regression models T-tests (also ANOVA) ANCOVA models Interactions very basic model selection basic GLMs: the logistic regression model Bonus topic: fitting non-linear regression models using nls Bonus topic: fitting custom maximum-likelihood models using optim R’s built-in statistical modeling framework is pretty intuitive and comprehensive. R has gained popularity as a statistics software and is commonly used both in academia and governmental resource agencies. This popularity is likely a result of its power, flexibility, intuitive nature, and price (free!). For many students, this chapter may be the one that is most immediately useful. IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapters 1 or 2, you are recommended to walk through the material found in those chapters before proceeding to this material. Also note that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this document. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch3.R and save it in the directory C:/Users/YOU/Documents/R-Workshop/Chapter3. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. 3.1 Review: The General Linear Model This is a family of models that allows you to determine the relationship (if any) between some continuous response variable (\\(y\\)) and some predictor variable(s) (\\(x_n\\)) and is often written as: \\[y_i=\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_j x_{ij}+ ... + \\beta_n x_{in} + \\varepsilon_i; \\varepsilon_i \\sim N(0,\\sigma)\\] The predictor variable(s) can be either categorical (i.e., grouping variables used in ANOVA, t-test, etc.), continuous (regression), or a combination of categorical and continuous (ANCOVA). The main focus is to estimate the coefficients (\\(\\beta\\)), and in some cases it is to determine if they are “significantly” different from the value given by some null hypothesis. The model makes several assumptions about the residuals1 to obtain estimates of the coefficients. For reliable inference, the residuals must: Be independent Be normally-distributed Have constant variance across range of the x-axis In R, the general linear model is fitted using the lm function. Here’s the basic syntax is lm(y ~ x, data = dat)2; it says: “fit a model with y as the response variable and x as the sole predictor variable, look for the variables x and y in a data frame called dat, and store the results in a new object called fit”. 3.2 Simple Linear Regression Download the data set sockeye.csv from GitHub and place it in your working directory. This is the same data set you used in Exercise 2, see that section for more details on the different variables. Read these data into R: dat = read.csv(&quot;sockeye.csv&quot;) head(dat) ## year type weight fecund egg_size survival ## 1 1991 hatch NA NA NA NA ## 2 1992 hatch NA NA NA NA ## 3 1993 hatch 1801 2182 12.25 46.58 ## 4 1994 hatch 1681 2134 7.92 50.98 ## 5 1995 hatch 2630 1576 21.61 68.06 ## 6 1996 hatch 2165 2171 8.74 63.43 To fit a regression model using lm, but x and y must be continous (numeric) variables. In the data set dat, two such variables are the weight and fecund. Fit a regression model where you link the average fecundity of an individual to the average weight of an individual by treating years as replicate data points. Ignore for now that the fish come from two sources: hatchery and wild origin. fit1 = lm(fecund ~ weight, data = dat) If you run just the fit1 object, you will see the model you ran along with the coefficient estimates of the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)): fit1 ## ## Call: ## lm(formula = fecund ~ weight, data = dat) ## ## Coefficients: ## (Intercept) weight ## 1874.6496 0.2104 If \\(x_{i1}\\) is weight, then the coefficients are interpretted as: \\(\\beta_0\\): the y-intercept (mean fecund at zero weight) \\(\\beta_1\\): the slope (change in fecund for one unit change in weight) For more information about the model fit, you can use the summary function: summary(fit1) ## ## Call: ## lm(formula = fecund ~ weight, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -873.67 -389.28 -71.65 482.96 1041.24 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1874.6496 269.4369 6.958 4.33e-08 *** ## weight 0.2104 0.1803 1.167 0.251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 500.6 on 35 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.03745, Adjusted R-squared: 0.009945 ## F-statistic: 1.362 on 1 and 35 DF, p-value: 0.2511 Again the coefficient estimates are shown, but now you see the uncertainty on the parameter estimates (standard errors), the test statistic, and the p-value testing the null hypothesis that each coefficient has a zero value. Here you can see that the p-value does not support rejection of the null hypothesis that the slope is zero. You can see the residual standard error (variability of data around the line and the estimate of \\(\\sigma\\)), the \\(R^2\\) value (the proportion of variation in fecund explained by variation in weight), and the p-value of the overall model. You can easily see the model fit by using the abline function. Make a new plot and add the fitted regression line: plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = 16, cex = 1.5) abline(fit1) It fits, but not very well. It seems there are two groups: one with data points mostly above the line and one with data points mostly below the line. You’ll now run a new model to get at this. 3.3 ANOVA: Categorical predictors ANOVA models attempt to determine if the means of different groups are different. You can fit them in the same basic lm framework. But first, notice that: class(dat$type); levels(dat$type) ## [1] &quot;factor&quot; ## [1] &quot;hatch&quot; &quot;wild&quot; tells you the type variable is a factor. It has levels of &quot;hatch&quot; and &quot;wild&quot; which indicate the origin of the adult spawning fish sampled each year. If you pass lm a predictor variable with a factor class, the R will automatically fit it as an ANOVA model. See Section 1.5 for more details on factors. Factors have an explicit ordering of the levels. By default, this ordering happens alphabetically: if your factor has levels &quot;a&quot;, &quot;b&quot;, and &quot;c&quot;, they will be assigned the order of 1, 2 and 3, respectively. You can always see how R is ordering your factor by doing something similar to this: pairs = cbind( as.character(dat$type), as.numeric(dat$type) ) head(pairs); tail(pairs) ## [,1] [,2] ## [1,] &quot;hatch&quot; &quot;1&quot; ## [2,] &quot;hatch&quot; &quot;1&quot; ## [3,] &quot;hatch&quot; &quot;1&quot; ## [4,] &quot;hatch&quot; &quot;1&quot; ## [5,] &quot;hatch&quot; &quot;1&quot; ## [6,] &quot;hatch&quot; &quot;1&quot; ## [,1] [,2] ## [39,] &quot;wild&quot; &quot;2&quot; ## [40,] &quot;wild&quot; &quot;2&quot; ## [41,] &quot;wild&quot; &quot;2&quot; ## [42,] &quot;wild&quot; &quot;2&quot; ## [43,] &quot;wild&quot; &quot;2&quot; ## [44,] &quot;wild&quot; &quot;2&quot; The functions as.character and as.numeric are coersion functions: they attempt to change the way something is interpretted. Notice that the level &quot;hatch&quot; is assigned the order 1 because it comes before &quot;wild&quot; alphebetically. The first level is termed the reference level because it is the group that all other levels are compared to when fitting a model. You can change the reference level using dat$type_rlvl = relevel(dat$type, ref = &quot;wild&quot;). You are now ready to fit the ANOVA model, which will measure the size of the difference in the mean fecund between different levels of the factor type: fit2 = lm(fecund ~ type, data = dat) Think of this model as being written as: \\[y_i=\\beta_0 + \\beta_1 x_{i1} + \\varepsilon_i\\] and assume that \\(x_{i1} = 0\\) if observation \\(i\\) is from &quot;hatch&quot; fish and \\(x_{i1} = 1\\) if observation \\(i\\) is from &quot;wild&quot; fish. In that case: \\(\\beta_0\\) (the intercept) is interpretted as the mean fecund for the &quot;hatch&quot; level and \\(\\beta_1\\) is interpretted as the difference in mean fecund between the &quot;wild&quot; level and the &quot;hatch&quot; level. So when you run coef(fit2) to extract the coefficient estimates and get: ## (Intercept) typewild ## 1846.2500 713.3971 you see that the mean fecundity of hatchery fish is about 1846 eggs and that the average wild fish has about 713 more eggs than the average hatchery fish across all years. The fact that the p-value associated with the typewild coefficient when you run summary(fit2) is less than 0.05 indicates that there is statistical evidence that the difference in means is not zero. 3.4 ANCOVA: Continuous and categorical predictors Now that you have seen that hatchery and wild fish tend to separate along the fecundity axis (as evidenced by the ANOVA results above), you would like to include this in your original regression model. You will fit two lines within the same model: one for hatchery fish and one for wild fish. This model is called an ANCOVA model and looks like this: \\[y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i\\] If \\(x_{i1}\\) is type coded with 0’s and 1’s as in 3.3 and \\(x_{i2}\\) is weight, then the coefficients are interpretted as: \\(\\beta_0\\): the y-intercept of the &quot;hatch&quot; level (the reference level) \\(\\beta_1\\): the difference in y-intercept between the &quot;wild&quot; level and the &quot;hatch&quot; level. \\(\\beta_2\\): the slope of both lines (this model assumes the lines have common slopes, i.e., that the lines are parallel) You can fit this model and extract the coefficents table from the summary: fit3 = lm(fecund ~ type + weight, data = dat) summary(fit3)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1039.6417645 175.0992184 5.937444 1.038268e-06 ## typewild 866.9909998 96.2473387 9.007948 1.578239e-10 ## weight 0.5173716 0.1051039 4.922477 2.164460e-05 And you can plot the fit. Study this code to make sure you know what each is doing. Use what you know about the meanings of the three coefficients to decipher the two abline commands. Remember that abline takes takes two arguments: a is the intercept and b is the slope. plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = ifelse(dat$type == &quot;hatch&quot;, 1, 16), cex = 1.5) abline(coef(fit3)[c(1,3)], lty = 2) abline(sum(coef(fit3)[c(1,2)]), coef(fit3)[3]) legend(&quot;bottom&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), pch = c(1,16), lty = c(2,1), col = &quot;grey&quot;, pt.cex = 1.5, bty = &quot;n&quot;, horiz = T) 3.5 Interactions Above, you have included an additional predictor variable (and parameter) in your model to help explain variation in the fecund variable. However, you have assumed that the effect of weight on fecundity is common between hatchery and wild fish (note the parallel lines in the figure above). You may have reason to believe that the effect of weight depends on the origin of the fish, e.g., wild fish may tend to accumulate more eggs than hatchery fish for the same increase in weight. Cases where the magnitude of the effect depends on the value of another predictor variable are known as “interactions”. You can write the interactive ANCOVA model like this: \\[y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i1} x_{i2} + \\varepsilon_i\\] If \\(x_{i1}\\) is type coded with 0’s and 1’s as in 3.3 and \\(x_{i2}\\) is weight, then the coefficients are interpretted as: \\(\\beta_0\\): the y-intercept of the &quot;hatch&quot; level (the reference level) \\(\\beta_1\\): the difference in y-intercept between the &quot;wild&quot; level and the &quot;hatch&quot; level. \\(\\beta_2\\): the slope of the &quot;hatch&quot; level \\(\\beta_3\\): the difference in slope between the &quot;wild&quot; level and the &quot;hatch&quot; level. You can fit this model: fit4 = lm(fecund ~ type + weight + type:weight, data = dat) # or # fit4 = lm(fecund ~ type * weight, data = dat) The first option above is more clear in its statement, but both do the same thing. You can plot the fit. Study these lines to make sure you know what each is doing. Use what you know about the meanings of the four coefficients to decipher the two abline commands. plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = ifelse(dat$type == &quot;hatch&quot;, 1, 16), cex = 1.5) abline(coef(fit4)[c(1,3)], lty = 2) abline(sum(coef(fit4)[c(1,2)]), sum(coef(fit4)[c(3,4)])) legend(&quot;bottom&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), pch = c(1,16), lty = c(2,1), col = &quot;grey&quot;, pt.cex = 1.5, bty = &quot;n&quot;, horiz = T) Based on the coefficients table: summary(fit4)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1175.7190847 174.545223 6.7358995 1.125255e-07 ## typewild -42.8721082 398.980751 -0.1074541 9.150794e-01 ## weight 0.4300894 0.105592 4.0731253 2.732580e-04 ## typewild:weight 0.7003389 0.299104 2.3414560 2.539681e-02 It seems that fish of the different origins have approximately the same intercept, but that their slopes are quite different. 3.6 Model Selection: AIC You have now fitted four different models, each that makes different claims about how you can predict the fecundity of a given sockeye salmon at Redfish Lake. If you are interested in determining which of these models you should use for prediction, you need to use model selection. Model selection attempts to find the model that is likely to have the smallest out-of-sample prediction error (i.e., future predictions will be close to what actually happens). One model selection metric is the AIC3. Lower AIC values mean the model should have better predictive performance. Compare the four models you fitted with AIC: AIC(fit1, fit2, fit3, fit4) ## df AIC ## fit1 3 568.9166 ## fit2 3 543.6914 ## fit3 4 525.7834 ## fit4 5 522.0968 In general, AIC values that are different by more than 2 units are interpretted as having importantly different predictive performance. Based on this very quick-and-dirty analysis, it seems that in predicting future fecundity, you would want to use the interactive ANCOVA model. 3.7 An Example GLM: Logistic Regression The models you fitted above were called “general linear models”. They all made the assumption that the residuals (\\(\\varepsilon_i\\)) are normally-distributed. Often times data and analyses do not follow this assumption. For these cases you often move to the broader family of statistical models known as “generalized linear models”4. One example is in the case of binary data. Binary data have two outcomes, e.g., success/failure, lived/died, male/female, spawned/gravid, happy/sad, etc. If you wish to predict how the probability of one outcome over the other changes depending on some other variable, then you need to use the logistic regression model, which is written as: \\[logit(p_i)=\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_j x_{ij}+ ... + \\beta_n x_{in}; y_i \\sim Bernoulli(p_i)\\] Where \\(p_i\\) is the probability that observation \\(y_i\\) was a success (\\(y_i = 1\\)). The \\(logit(p_i)\\) is the link function that links the linear parameter scale to the data scale. It constrains the value of \\(p_i\\) to be between 0 and 1 regardless of the values of the \\(\\beta\\) coefficients. The logit link function does this: \\[logit(p_i) = log\\left(\\frac{p_i}{1-p_i}\\right)\\] which is the natural logarithm of the odds, a measure of how likely the event is to happen relative to it not happening. Make an R function to calculate the logit transformation: logit = function(p) { log(p/(1 - p)) } If you have the result of logit(p[i]) (which is given by the \\(\\beta\\) coefficients and the \\(x_ij\\) data) and need to get p[i], you can apply the inverse logit function: \\[expit(lp_i)=\\frac{e^{lp_i}}{1 + e^{lp_i}}\\] Make a function for the inverse logit transformation: expit = function(lp) { # lp stands for logit(p) exp(lp)/(1 + exp(lp)) } Because of the logit link function, the coefficients have different interpretations than in the previous models you’ve fitted in this chapter: they are expressed in terms of log odds. Fit a logistic regression model to the sockeye salmon data. None of the variables of interest are binary, but you can create one. Look at the variable dat$survival. This is the average % survival of all eggs laid that make it to the “eye-egg” stage. Create a new variable binary which takes on a 0 if dat$survival is less than 70% and a 1 otherwise: dat$binary = ifelse(dat$survival &lt; 70, 0, 1) This will be your response variable and your model will predict the probability that it is a 1. A basic model would have just weight as the predictor variable: fit1 = glm(binary ~ weight, data = dat, family = binomial) summary(fit1)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.363441330 1.76943946 2.466002 0.01366306 ## weight -0.002819271 0.00125243 -2.251040 0.02438303 The coefficients are interpretted as: \\(\\beta_0\\): the log odds of success for a fish with zero weight (which is not all that important). \\(e^{\\beta_0}\\) is the odds of success for fish with zero weight, and \\(expit(e^{\\beta_0})\\) is the probability of success forr fish with zero weight. Remember “success” is defined as having at least 70% egg survival to the stage of interest. \\(\\beta_1\\): the additive effect of fish weight on the log odds of success. \\(e^{\\beta_1}\\) is interpretted as the ratio of the odds at two consective weights (e.g., 1500 and 1501). Claims about \\(e^{\\beta_1}\\) are made as “for every one gram increase in weight, success became \\(e^{\\beta_1}\\) times as likely to happen”. You can predict the probability of success any weight using \\(expit(\\beta_0 + \\beta_1 weight)\\) You can plot the fitted model: # create a sequence of weights to predict at wt_seq = seq(min(dat$weight, na.rm = T), max(dat$weight, na.rm = t), length = 100) # extract the coefficients and get p p = expit(coef(fit1)[1] + coef(fit1)[2] * wt_seq) # plot the relationship plot(p ~ wt_seq, type = &quot;l&quot;, lwd = 3, ylim = c(0,1), las = 1) Fit another model comparing the success rates between hatchery and wild fish: fit2 = glm(binary ~ type, data = dat, family = binomial) summary(fit2)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2006707 0.4494666 -0.4464641 0.6552620 ## typewild 1.3793257 0.7272845 1.8965421 0.0578884 An easier way to obtain the predicted probability is by using the predict function: predict(fit2, newdata = data.frame(type = c(&quot;hatch&quot;, &quot;wild&quot;)), type = &quot;response&quot;) ## 1 2 ## 0.4500000 0.7647059 This plugs in the two possible values of the predictor variable and asks for the fitted probabilities. Incorporate the origin type into your original model: fit3 = glm(binary ~ type + weight, data = dat) and obtain/plot the fitted probabilities for each group: p_hatch = predict( fit3, newdata = data.frame(type = &quot;hatch&quot;, weight = wt_seq), type = &quot;response&quot; ) p_wild = predict( fit3, newdata = data.frame(type = &quot;wild&quot;, weight = wt_seq), type = &quot;response&quot; ) plot(p_wild ~ wt_seq, type = &quot;l&quot;, lwd = 3, lty = 1, ylim = c(0,1), las =1, xlab = &quot;Weight (g)&quot;, ylab = &quot;Pr(&gt;70% Egg Survival)&quot; ) lines(p_hatch ~ wt_seq, lwd = 3, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), lty = c(2,1), lwd = 3, bty = &quot;n&quot;) Look for an interaction (all the code is the same except use glm(binary ~ type * weight) instead of glm(binary ~ type + weight) and change everything to fit4 instead of fit3). You may have noticed that you just did the same analysis with binary as the response instead of fecund. Perform an AIC analysis to determine which model is likely to be best for prediction: AIC(fit1, fit2, fit3, fit4) ## df AIC ## fit1 2 45.40720 ## fit2 2 50.07577 ## fit3 4 50.42090 ## fit4 3 46.00622 Oddly enough, the two best models are the simplest one and the most complex one, with fit1 being the best, but not by a large margin. 3.8 Probability Distributions The residuals (\\(\\varepsilon_i\\)) are the difference between the data point \\(y_i\\) and the model prediction \\(\\hat{y}_i\\): \\(\\varepsilon_i=y_i-\\hat{y}_i\\)↩ This should look familar from Section 2.4↩ Akaike’s Information Criterion. GIVE A CITATION HERE↩ General linear models are a member of this family↩ "]
]
